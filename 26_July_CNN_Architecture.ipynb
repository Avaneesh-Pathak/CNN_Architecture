{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263c25fa",
   "metadata": {},
   "source": [
    "Describe the benifits and Purpose of pooling in CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b1d1e",
   "metadata": {},
   "source": [
    "pooling in CNNs serves to reduce dimensionality, increase robustness to translations, select important features, reduce overfitting, and make the network more memory and computationally efficient. These benefits contribute to the overall effectiveness of CNNs in various computer vision tasks, such as image classification and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdb562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11dff7a",
   "metadata": {},
   "source": [
    "Max Pooling is a pooling operation where, for each region of the input feature map, the maximum value is retained while discarding the rest.\n",
    "\n",
    "Min Pooling is a less common pooling operation where, for each region of the input feature map, the minimum value is retained while discarding the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd10f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d546452",
   "metadata": {},
   "source": [
    "padding in CNNs is a crucial technique that helps preserve spatial information, prevent information loss, control the output size, and maintain consistency in network architectures. It allows CNNs to effectively capture features and patterns from various positions within the input data, making it a vital component for tasks like image recognition, object detection, and segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a55c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd15d29a",
   "metadata": {},
   "source": [
    "Valid (No Padding): In the \"valid\" padding mode, no padding is added to the input data. This results in feature maps that are smaller than the input, as the convolution operation reduces the spatial dimensions of the data. Valid padding is used when you want to perform aggressive dimensionality reduction in your CNN.\n",
    "\n",
    "Same (Zero Padding): In the \"same\" padding mode, padding is added in such a way that the output feature map has the same spatial dimensions as the input. If the convolution filter has a size of FxF (F is an odd number), then (F-1)/2 pixels are added to each side of the input, typically filled with zeros. This ensures that the convolution operation does not change the spatial dimensions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e192e",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b4618",
   "metadata": {},
   "source": [
    "LeNet-5 is a historic convolutional neural network (CNN) architecture developed by Yann LeCun in the 1990s. It was designed for handwritten digit recognition, particularly on the MNIST dataset. LeNet-5 consists of two convolutional layers with average pooling, followed by three fully connected layers. It introduced key concepts such as convolution, pooling, and trainable parameters, setting the stage for modern deep learning and CNN architectures. Despite its simplicity compared to contemporary models, LeNet-5 was a groundbreaking step in the development of deep neural networks for image recognition, achieving excellent performance on handwritten digit recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae994a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb36595",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in the early 1990s. It was designed for the task of handwritten digit recognition, specifically for recognizing characters from the MNIST dataset. LeNet-5 played a significant role in the development of deep learning and convolutional neural networks, laying the foundation for modern CNN architectures. Here's a concise overview of the LeNet-5 architecture:\n",
    "\n",
    "**1. Input Layer:** LeNet-5 takes grayscale images as input, typically with dimensions of 32x32 pixels, which are common for the MNIST dataset. The network can also handle larger input sizes.\n",
    "\n",
    "**2. Convolutional Layers:**\n",
    "   - LeNet-5 consists of two convolutional layers followed by average pooling layers.\n",
    "   - The first convolutional layer applies six convolutional filters (kernels) with a size of 5x5 pixels. Each filter computes a set of feature maps.\n",
    "   - The second convolutional layer applies 16 filters of size 5x5 to the feature maps produced by the first layer.\n",
    "   - A non-linear activation function (commonly tanh or sigmoid) is applied after each convolution operation.\n",
    "\n",
    "**3. Average Pooling Layers:**\n",
    "   - After each convolutional layer, LeNet-5 employs average pooling layers.\n",
    "   - The first pooling layer has a size of 2x2 and reduces the spatial dimensions of the feature maps.\n",
    "   - The second pooling layer also has a 2x2 size and further reduces the spatial dimensions.\n",
    "\n",
    "**4. Fully Connected Layers:**\n",
    "   - Following the convolutional and pooling layers, LeNet-5 has three fully connected layers.\n",
    "   - The first fully connected layer has 120 neurons.\n",
    "   - The second fully connected layer has 84 neurons.\n",
    "   - The third fully connected layer is the output layer, which typically has 10 neurons (one for each digit class in MNIST).\n",
    "\n",
    "**5. Activation Functions:**\n",
    "   - Sigmoid or tanh activation functions were commonly used in the hidden layers.\n",
    "   - The output layer typically uses a softmax activation function to produce class probabilities.\n",
    "\n",
    "**6. Training and Optimization:**\n",
    "   - LeNet-5 was trained using gradient-based optimization methods, such as stochastic gradient descent (SGD).\n",
    "   - It used the cross-entropy loss function for training.\n",
    "\n",
    "**7. Achievements:**\n",
    "   - LeNet-5 was one of the earliest successful applications of CNNs for image recognition tasks.\n",
    "   - It achieved excellent performance on handwritten digit recognition, with error rates lower than previous methods.\n",
    "\n",
    "While LeNet-5's architecture may appear relatively simple compared to modern CNNs, its innovations in convolutional and pooling layers, as well as the use of trainable parameters, were pioneering steps in the development of deep learning. LeNet-5 laid the foundation for subsequent CNN architectures and demonstrated the power of deep neural networks for computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc45208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f9a4d4",
   "metadata": {},
   "source": [
    "LeNet-5, a pioneering convolutional neural network (CNN) architecture, offers advantages such as effective feature extraction, parameter efficiency, and translation invariance, making it well-suited for tasks like handwritten digit recognition. However, it has limitations, including limited capacity for complex tasks, a preference for small input sizes, the use of non-linearities prone to vanishing gradients, and specialization for specific datasets. LeNet-5's purpose was to excel at simple image classification with interpretable features, while its limitations prompted the development of more advanced CNNs capable of handling larger datasets and complex computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e2262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00352b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04093a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 15s 19ms/step - loss: 0.2731 - accuracy: 0.9170 - val_loss: 0.1031 - val_accuracy: 0.9688\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 16s 21ms/step - loss: 0.0876 - accuracy: 0.9734 - val_loss: 0.0792 - val_accuracy: 0.9768\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0616 - accuracy: 0.9810 - val_loss: 0.0671 - val_accuracy: 0.9801\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 17s 23ms/step - loss: 0.0484 - accuracy: 0.9849 - val_loss: 0.0514 - val_accuracy: 0.9847\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 15s 20ms/step - loss: 0.0400 - accuracy: 0.9873 - val_loss: 0.0596 - val_accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0319 - accuracy: 0.9896 - val_loss: 0.0475 - val_accuracy: 0.9865\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 17s 22ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.0485 - val_accuracy: 0.9854\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0233 - accuracy: 0.9927 - val_loss: 0.0470 - val_accuracy: 0.9866\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0482 - val_accuracy: 0.9860\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0508 - val_accuracy: 0.9865\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0493 - accuracy: 0.9863\n",
      "Test Accuracy: 98.63%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the LeNet-5 architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77929855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d19349e",
   "metadata": {},
   "source": [
    "AlexNet is a pioneering convolutional neural network (CNN) architecture that revolutionized computer vision and deep learning. Key features of the architecture include five convolutional layers with ReLU activation, max-pooling layers, local response normalization (LRN), and three fully connected layers. Dropout and softmax activation are used to prevent overfitting and produce class probabilities. AlexNet introduced GPU acceleration, data augmentation, and cross-validation techniques, setting new standards in image classification accuracy. Its success marked a pivotal moment in the development of deep learning, inspiring further advances in CNN architectures and their applications in computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "670f6fa3",
   "metadata": {},
   "source": [
    "Deep Convolutional Layers: AlexNet was one of the first CNN architectures to employ a deep stack of convolutional layers. Prior to AlexNet, most CNNs were relatively shallow. AlexNet used five convolutional layers, which allowed it to capture hierarchical features in the data. The depth of the network contributed to its ability to learn complex and discriminative features.\n",
    "\n",
    "ReLU Activation: AlexNet popularized the use of the Rectified Linear Unit (ReLU) activation function. ReLU introduces non-linearity into the network and helps mitigate the vanishing gradient problem during training. This non-linearity allowed the network to learn more complex representations of the data and enabled faster convergence.\n",
    "\n",
    "Local Response Normalization (LRN): AlexNet incorporated LRN layers after the first and second convolutional layers. LRN is a form of normalization that enhances the contrast between features by normalizing responses within local neighborhoods of the feature maps. This normalization technique contributed to the network's ability to generalize better.\n",
    "\n",
    "Max-Pooling Layers: Max-pooling layers were used to reduce the spatial dimensions of the feature maps after the convolutional layers. Max-pooling provides translation invariance and helps reduce computational complexity while preserving important features.\n",
    "\n",
    "Dropout: AlexNet introduced the use of dropout in the fully connected layers. Dropout is a regularization technique that randomly drops a fraction of neurons during training. It helps prevent overfitting by promoting robustness and diversity in the learned features.\n",
    "\n",
    "Parallelization with GPUs: AlexNet took advantage of powerful GPUs for training, enabling faster and more efficient computations. This parallelization greatly accelerated training times and made it feasible to train deep neural networks on large datasets.\n",
    "\n",
    "Data Augmentation: Data augmentation techniques, such as cropping and flipping, were used to increase the diversity of the training data. This helped reduce overfitting and improved the model's ability to generalize to unseen data.\n",
    "\n",
    "Cross-Validation: Cross-validation was employed to evaluate model performance and prevent overfitting. By splitting the data into training and validation sets and iteratively training on different subsets, AlexNet ensured that the model was robust and not overly tailored to the training data.\n",
    "\n",
    "Large-Scale Training Data: AlexNet was trained on a massive dataset, including over a million labeled images from ImageNet, which allowed it to learn a rich set of features and achieve high recognition accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537209d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a761745d",
   "metadata": {},
   "source": [
    "the convolutional layers in AlexNet extract features of varying complexity, the pooling layers reduce spatial dimensions and introduce translation invariance, and the fully connected layers serve as a powerful classifier. Together, these layer types form a hierarchical architecture that can learn and recognize complex patterns and objects in images. The innovative use of these layers, along with other architectural advancements, contributed to AlexNet's groundbreaking performance in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e3dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c773af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the AlexNet architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(256, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(layers.Conv2D(384, (3, 3), activation='relu'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55125f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3ee53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1984af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84221d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cc76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d7803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35355043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
